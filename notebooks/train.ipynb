{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/\n",
    "# Install customized linc-detector faster-rcnn\n",
    "!pip install --upgrade --force-reinstall git+https://github.com/linc-lion/LINC-detector.git@5d650eec07a79605c37618dce4e2648d71fa921a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pycocotools\n",
    "!pip install --upgrade --force-reinstall cython\n",
    "!pip install --upgrade --force-reinstall -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from linc.detector.models import detection\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from linc.detector.helper.coco_utils import get_coco  # get_coco_kp\n",
    "from linc.detector.helper.group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from linc.detector.helper.engine import train_one_epoch, evaluate\n",
    "from linc.detector.helper import utils\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "* The training code is extracted from [pytorch example](https://github.com/pytorch/vision/blob/528651a031a08f9f97cc75bd619a326387708219/references/detection/train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to COCO formatted object detection dataset\n",
    "data_path = '../datasets/coco_all_but_ws_and_fb/'  \n",
    "\n",
    "# Ignorable arguments\n",
    "epochs = 2 # 35\n",
    "save_every_num_epochs = None  # Optional\n",
    "evaluate_every_num_epochs = 2\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "lr_steps = [10, 11]\n",
    "lr_gamma = 0.1\n",
    "batch_size = 3\n",
    "workers = 8\n",
    "run_name = \"linc-detector-tensorboard\"  # Optional, str used to name Tensorboard summaries\n",
    "num_draw_predictions = 5\n",
    "draw_threshold = 0.5\n",
    "aspect_ratio_group_factor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create summary writer for Tensorboard\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Summary folder 'linc-detector-tensorboard' already exists. Overwrite it [yes, y / no, n]? y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create datasets\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Categorizing into 32 classes\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Create samplers\n",
      "Using [0, 1.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [15  1]\n",
      "Create dataloaders\n",
      "Create model\n",
      "Device: cpu\n",
      "Start training\n",
      "Epoch: [0]  [0/5]  eta: 0:03:40  lr: 0.002507  loss: 4.3316 (4.3316)  loss_classifier: 3.4500 (3.4500)  loss_box_reg: 0.0403 (0.0403)  loss_objectness: 0.6934 (0.6934)  loss_rpn_box_reg: 0.1479 (0.1479)  time: 44.1299  data: 2.7619  max mem: 0\n",
      "Epoch: [0]  [4/5]  eta: 0:00:39  lr: 0.010000  loss: 4.2557 (3.9952)  loss_classifier: 3.3884 (3.1699)  loss_box_reg: 0.0403 (0.0453)  loss_objectness: 0.6934 (0.6925)  loss_rpn_box_reg: 0.0859 (0.0875)  time: 39.0331  data: 0.5918  max mem: 0\n",
      "Epoch: [0] Total time: 0:03:15 (39.0700 s / it)\n",
      "Epoch time 195.39691996574402\n",
      "Test:  [0/5]  eta: 0:00:33  model_time: 6.0025 (6.0025)  evaluator_time: 0.0014 (0.0014)  time: 6.6203  data: 0.6161  max mem: 0\n",
      "Test:  [4/5]  eta: 0:00:05  model_time: 4.8591 (5.0407)  evaluator_time: 0.0007 (0.0008)  time: 5.1712  data: 0.1236  max mem: 0\n",
      "Test: Total time: 0:00:25 (5.1908 s / it)\n",
      "Averaged stats: model_time: 4.8591 (5.0407)  evaluator_time: 0.0007 (0.0008)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Epoch: [1]  [0/5]  eta: 0:04:13  lr: 0.010000  loss: 1.6950 (1.6950)  loss_classifier: 0.8396 (0.8396)  loss_box_reg: 0.0688 (0.0688)  loss_objectness: 0.6658 (0.6658)  loss_rpn_box_reg: 0.1208 (0.1208)  time: 50.6384  data: 7.7937  max mem: 0\n",
      "Epoch: [1]  [4/5]  eta: 0:00:41  lr: 0.010000  loss: 1.3552 (1.2385)  loss_classifier: 0.6822 (0.6029)  loss_box_reg: 0.0688 (0.0707)  loss_objectness: 0.4591 (0.4779)  loss_rpn_box_reg: 0.0711 (0.0870)  time: 41.5083  data: 1.5859  max mem: 0\n",
      "Epoch: [1] Total time: 0:03:27 (41.5506 s / it)\n",
      "Epoch time 207.77176809310913\n",
      "Training time 0:07:09\n"
     ]
    }
   ],
   "source": [
    "# Training code is based on \n",
    "\n",
    "print(\"Create summary writer for Tensorboard\")\n",
    "if run_name:\n",
    "    log_dir_path = f\"{run_name}\" if run_name else None\n",
    "    if os.path.isdir(log_dir_path):\n",
    "        delete = input(f\"Summary folder '{log_dir_path}' already exists. Overwrite it [yes, y / no, n]?\")\n",
    "        if delete in ('yes', 'y'):\n",
    "            shutil.rmtree(log_dir_path)\n",
    "        else:\n",
    "            print(f\"Chose another run name or delete the folder then!\")\n",
    "            exit()\n",
    "else:\n",
    "    log_dir_path = None\n",
    "writer = SummaryWriter(log_dir=log_dir_path)\n",
    "\n",
    "# Add some useful text summaries (Tensorboard uses markdown to render text).\n",
    "# writer.add_text('Command executed', f\"python {' '.join(sys.argv)}\")\n",
    "# writer.add_text('Arguments', str(args).replace(\", \", \",  \\n\").replace(\"Namespace(\", \"\").replace(\")\", \"\"))\n",
    "\n",
    "print(\"Create datasets\")\n",
    "dataset, num_classes, label_names = get_coco(data_path, image_set='train')\n",
    "print(f\"Categorizing into {num_classes} classes\")\n",
    "dataset_test, _, _ = get_coco(data_path, image_set='val')\n",
    "\n",
    "print(\"Create samplers\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "\n",
    "print(\"Create dataloaders\")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_sampler=train_batch_sampler, \n",
    "                                          num_workers=workers, \n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, \n",
    "                                               batch_size=1,\n",
    "                                               sampler=test_sampler, \n",
    "                                               num_workers=workers, \n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "print(\"Create model\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "model = detection.__dict__['fasterrcnn_resnet50_fpn'](num_classes=num_classes, pretrained=False)\n",
    "model.to(device)\n",
    "model_without_ddp = model\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=lr, \n",
    "                            momentum=momentum, \n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                    milestones=lr_steps, \n",
    "                                                    gamma=lr_gamma)\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    train_one_epoch(\n",
    "        model, optimizer, data_loader, device, epoch, 20, writer, label_names\n",
    "    )\n",
    "    print(f\"Epoch time {time.time() - start_epoch}\")\n",
    "    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if save_every_num_epochs and epoch % save_every_num_epochs == 0:\n",
    "        utils.save_on_master({\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'label_names': label_names},\n",
    "            os.path.join(writer.log_dir, 'model_{}.pth'.format(epoch))\n",
    "        )\n",
    "\n",
    "    if epoch % evaluate_every_num_epochs == 0:\n",
    "        evaluate(\n",
    "            model, data_loader_test, epoch, writer, draw_threshold,\n",
    "            label_names, num_draw_predictions, device=device\n",
    "        )\n",
    "\n",
    "# Save after training is done\n",
    "utils.save_on_master({\n",
    "    'model': model_without_ddp.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "    'label_names': label_names},\n",
    "    os.path.join(writer.log_dir, 'model_finished.pth')\n",
    ")\n",
    "\n",
    "writer.close()\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip uninstall tb-nightly tensorboardX tensorboard -y\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-15f74b713e84bc36\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-15f74b713e84bc36\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir linc-detector-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
