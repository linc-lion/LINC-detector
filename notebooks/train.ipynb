{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/\n",
    "# Install customized linc-detector faster-rcnn\n",
    "!pip install --upgrade --force-reinstall git+https://github.com/linc-lion/LINC-detector.git@ba36a5bfa5ba7b9035977c02b1d8ed253f074e8d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pycocotools\n",
    "!pip install --upgrade --force-reinstall cython\n",
    "!pip install --upgrade --force-reinstall -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from linc.detector.models import detection\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from linc.detector.helper.coco_utils import get_coco  # get_coco_kp\n",
    "from linc.detector.helper.group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from linc.detector.helper.engine import train_one_epoch, evaluate\n",
    "from linc.detector.helper import utils\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "* The base training code is extracted from [pytorch example](https://github.com/pytorch/vision/blob/528651a031a08f9f97cc75bd619a326387708219/references/detection/train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to COCO formatted object detection dataset\n",
    "data_path = '../datasets/coco_all_but_ws_and_fb/'  \n",
    "\n",
    "# Ignorable arguments\n",
    "epochs = 2 # 35\n",
    "save_every_num_epochs = None  # Optional\n",
    "evaluate_every_num_epochs = 2\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "lr_steps = [10, 11]\n",
    "lr_gamma = 0.1\n",
    "batch_size = 3\n",
    "workers = 8\n",
    "run_name = \"linc-detector-tensorboard\"  # Optional, str used to name Tensorboard summaries\n",
    "num_draw_predictions = 5\n",
    "draw_threshold = 0.5\n",
    "aspect_ratio_group_factor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create summary writer for Tensorboard\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Summary folder 'linc-detector-tensorboard' already exists. Overwrite it [yes, y / no, n]? yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-11 12:27:06.642054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create datasets\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Categorizing into 32 classes\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Create samplers\n",
      "Using [0, 1.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [15  1]\n",
      "Create dataloaders\n",
      "Create model\n",
      "Device: cpu\n",
      "Start training\n",
      "Epoch: [0]  [0/5]  eta: 0:03:52  lr: 0.002507  loss: 4.2483 (4.2483)  loss_classifier: 3.4470 (3.4470)  loss_box_reg: 0.0203 (0.0203)  loss_objectness: 0.6902 (0.6902)  loss_rpn_box_reg: 0.0908 (0.0908)  time: 46.4494  data: 3.5685  max mem: 0\n",
      "Epoch: [0]  [4/5]  eta: 0:00:40  lr: 0.010000  loss: 4.1892 (4.1375)  loss_classifier: 3.4133 (3.3396)  loss_box_reg: 0.0203 (0.0224)  loss_objectness: 0.6896 (0.6885)  loss_rpn_box_reg: 0.0908 (0.0870)  time: 40.4812  data: 0.7419  max mem: 0\n",
      "Epoch: [0] Total time: 0:03:22 (40.5414 s / it)\n",
      "Epoch time 202.76696014404297\n",
      "Test:  [0/5]  eta: 0:00:36  model_time: 5.4164 (5.4164)  evaluator_time: 0.0019 (0.0019)  time: 7.2819  data: 1.8635  max mem: 0\n",
      "Test:  [4/5]  eta: 0:00:05  model_time: 4.8651 (4.9764)  evaluator_time: 0.0006 (0.0009)  time: 5.3633  data: 0.3731  max mem: 0\n",
      "Test: Total time: 0:00:27 (5.4216 s / it)\n",
      "Averaged stats: model_time: 4.8651 (4.9764)  evaluator_time: 0.0006 (0.0009)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Epoch: [1]  [0/5]  eta: 0:03:42  lr: 0.010000  loss: 3.2722 (3.2722)  loss_classifier: 2.5291 (2.5291)  loss_box_reg: 0.0160 (0.0160)  loss_objectness: 0.6713 (0.6713)  loss_rpn_box_reg: 0.0558 (0.0558)  time: 44.5704  data: 2.7262  max mem: 0\n",
      "Epoch: [1]  [4/5]  eta: 0:01:15  lr: 0.010000  loss: 1.4353 (1.6925)  loss_classifier: 0.6411 (1.0195)  loss_box_reg: 0.0554 (0.0637)  loss_objectness: 0.5020 (0.5162)  loss_rpn_box_reg: 0.1073 (0.0930)  time: 75.8183  data: 0.5765  max mem: 0\n",
      "Epoch: [1] Total time: 0:06:19 (75.8881 s / it)\n",
      "Epoch time 379.5007789134979\n",
      "Training time 0:10:09\n"
     ]
    }
   ],
   "source": [
    "# Training code is based on \n",
    "\n",
    "print(\"Create summary writer for Tensorboard\")\n",
    "if run_name:\n",
    "    log_dir_path = f\"{run_name}\" if run_name else None\n",
    "    if os.path.isdir(log_dir_path):\n",
    "        delete = input(f\"Summary folder '{log_dir_path}' already exists. Overwrite it [yes, y / no, n]?\")\n",
    "        if delete in ('yes', 'y'):\n",
    "            shutil.rmtree(log_dir_path)\n",
    "        else:\n",
    "            print(f\"Chose another run name or delete the folder then!\")\n",
    "            exit()\n",
    "else:\n",
    "    log_dir_path = None\n",
    "writer = SummaryWriter(log_dir=log_dir_path)\n",
    "\n",
    "# Add some useful text summaries (Tensorboard uses markdown to render text).\n",
    "# writer.add_text('Command executed', f\"python {' '.join(sys.argv)}\")\n",
    "# writer.add_text('Arguments', str(args).replace(\", \", \",  \\n\").replace(\"Namespace(\", \"\").replace(\")\", \"\"))\n",
    "\n",
    "print(\"Create datasets\")\n",
    "dataset, num_classes, label_names = get_coco(data_path, image_set='train')\n",
    "print(f\"Categorizing into {num_classes} classes\")\n",
    "dataset_test, _, _ = get_coco(data_path, image_set='val')\n",
    "\n",
    "print(\"Create samplers\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "\n",
    "print(\"Create dataloaders\")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_sampler=train_batch_sampler, \n",
    "                                          num_workers=workers, \n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, \n",
    "                                               batch_size=1,\n",
    "                                               sampler=test_sampler, \n",
    "                                               num_workers=workers, \n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "print(\"Create model\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "model = detection.__dict__['fasterrcnn_resnet50_fpn'](num_classes=num_classes, pretrained=False)\n",
    "model.to(device)\n",
    "model_without_ddp = model\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=lr, \n",
    "                            momentum=momentum, \n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                    milestones=lr_steps, \n",
    "                                                    gamma=lr_gamma)\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    train_one_epoch(\n",
    "        model, optimizer, data_loader, device, epoch, 20, writer, label_names\n",
    "    )\n",
    "    print(f\"Epoch time {time.time() - start_epoch}\")\n",
    "    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if save_every_num_epochs and epoch % save_every_num_epochs == 0:\n",
    "        utils.save_on_master({\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'label_names': label_names},\n",
    "            os.path.join(writer.log_dir, 'model_{}.pth'.format(epoch))\n",
    "        )\n",
    "\n",
    "    if epoch % evaluate_every_num_epochs == 0:\n",
    "        evaluate(\n",
    "            model, data_loader_test, epoch, writer, draw_threshold,\n",
    "            label_names, num_draw_predictions, device=device\n",
    "        )\n",
    "\n",
    "# Save after training is done\n",
    "utils.save_on_master({\n",
    "    'model': model_without_ddp.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "    'label_names': label_names},\n",
    "    os.path.join(writer.log_dir, 'model_finished.pth')\n",
    ")\n",
    "\n",
    "writer.close()\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12351), started 0:05:51 ago. (Use '!kill 12351' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aae25d7b795ed48d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aae25d7b795ed48d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir linc-detector-tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
