{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/\n",
    "# Install customized linc-detector faster-rcnn\n",
    "!pip install --upgrade --force-reinstall git+https://github.com/linc-lion/LINC-detector.git@5d650eec07a79605c37618dce4e2648d71fa921a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pycocotools\n",
    "!pip install --upgrade --force-reinstall cython\n",
    "!pip install --upgrade --force-reinstall -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from linc.detector.models import detection\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from linc.detector.helper.coco_utils import get_coco  # get_coco_kp\n",
    "from linc.detector.helper.group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
    "from linc.detector.helper.engine import train_one_epoch, evaluate\n",
    "from linc.detector.helper import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to COCO formatted object detection dataset\n",
    "data_path = '../datasets/coco_all_but_ws_and_fb/'  \n",
    "\n",
    "# Ignorable arguments\n",
    "epochs = 2 # 35\n",
    "save_every_num_epochs = None  # Optional\n",
    "evaluate_every_num_epochs = 2\n",
    "lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "lr_steps = [10, 11]\n",
    "lr_gamma = 0.1\n",
    "batch_size = 3\n",
    "workers = 8\n",
    "run_name = None  # Optional, str used to name Tensorboard summaries\n",
    "num_draw_predictions = 5\n",
    "draw_threshold = 0.5\n",
    "aspect_ratio_group_factor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create summary writer for Tensorboard\n",
      "Create datasets\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Categorizing into 32 classes\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Create samplers\n",
      "Using [0, 1.0, inf] as bins for aspect ratio quantization\n",
      "Count of instances per bin: [15  1]\n",
      "Create dataloaders\n",
      "Create model\n",
      "Device: cpu\n",
      "Start training\n",
      "Epoch: [0]  [0/5]  eta: 0:04:15  lr: 0.002507  loss: 4.3329 (4.3329)  loss_classifier: 3.4890 (3.4890)  loss_box_reg: 0.0445 (0.0445)  loss_objectness: 0.6952 (0.6952)  loss_rpn_box_reg: 0.1042 (0.1042)  time: 51.0650  data: 4.8009  max mem: 0\n",
      "Epoch: [0]  [4/5]  eta: 0:00:43  lr: 0.010000  loss: 4.2323 (4.1884)  loss_classifier: 3.4346 (3.3634)  loss_box_reg: 0.0436 (0.0429)  loss_objectness: 0.6947 (0.6937)  loss_rpn_box_reg: 0.0900 (0.0884)  time: 43.7501  data: 0.9915  max mem: 0\n",
      "Epoch: [0] Total time: 0:03:38 (43.7714 s / it)\n",
      "Epoch time 218.91928601264954\n",
      "Test:  [0/5]  eta: 0:00:36  model_time: 6.5730 (6.5730)  evaluator_time: 0.0016 (0.0016)  time: 7.2422  data: 0.6674  max mem: 0\n",
      "Test:  [4/5]  eta: 0:00:05  model_time: 5.3090 (5.5059)  evaluator_time: 0.0006 (0.0009)  time: 5.6498  data: 0.1340  max mem: 0\n",
      "Test: Total time: 0:00:28 (5.6733 s / it)\n",
      "Averaged stats: model_time: 5.3090 (5.5059)  evaluator_time: 0.0006 (0.0009)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "Epoch: [1]  [0/5]  eta: 0:04:38  lr: 0.010000  loss: 3.0117 (3.0117)  loss_classifier: 2.2544 (2.2544)  loss_box_reg: 0.0188 (0.0188)  loss_objectness: 0.6884 (0.6884)  loss_rpn_box_reg: 0.0502 (0.0502)  time: 55.6101  data: 8.2865  max mem: 0\n",
      "Epoch: [1]  [4/5]  eta: 0:01:10  lr: 0.010000  loss: 1.8629 (1.8354)  loss_classifier: 0.9688 (1.1644)  loss_box_reg: 0.0549 (0.0753)  loss_objectness: 0.4727 (0.4904)  loss_rpn_box_reg: 0.0969 (0.1054)  time: 70.3092  data: 1.6874  max mem: 0\n",
      "Epoch: [1] Total time: 0:05:51 (70.3414 s / it)\n",
      "Epoch time 351.74301314353943\n",
      "Training time 0:09:59\n"
     ]
    }
   ],
   "source": [
    "print(\"Create summary writer for Tensorboard\")\n",
    "if run_name:\n",
    "    log_dir_path = f\"runs/{run_name}\" if run_name else None\n",
    "    if os.path.isdir(log_dir_path):\n",
    "        delete = input(f\"Summary folder '{log_dir_path}' already exists. Overwrite it [yes, y / no, n]?\")\n",
    "        if delete in ('yes', 'y'):\n",
    "            shutil.rmtree(log_dir_path)\n",
    "        else:\n",
    "            print(f\"Chose another run name or delete the folder then!\")\n",
    "            exit()\n",
    "else:\n",
    "    log_dir_path = None\n",
    "writer = SummaryWriter(log_dir=log_dir_path)\n",
    "\n",
    "# Add some useful text summaries (Tensorboard uses markdown to render text).\n",
    "# writer.add_text('Command executed', f\"python {' '.join(sys.argv)}\")\n",
    "# writer.add_text('Arguments', str(args).replace(\", \", \",  \\n\").replace(\"Namespace(\", \"\").replace(\")\", \"\"))\n",
    "\n",
    "print(\"Create datasets\")\n",
    "dataset, num_classes, label_names = get_coco(data_path, image_set='train')\n",
    "print(f\"Categorizing into {num_classes} classes\")\n",
    "dataset_test, _, _ = get_coco(data_path, image_set='val')\n",
    "\n",
    "print(\"Create samplers\")\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "group_ids = create_aspect_ratio_groups(dataset, k=aspect_ratio_group_factor)\n",
    "train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, batch_size)\n",
    "\n",
    "print(\"Create dataloaders\")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                          batch_sampler=train_batch_sampler, \n",
    "                                          num_workers=workers, \n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, \n",
    "                                               batch_size=1,\n",
    "                                               sampler=test_sampler, \n",
    "                                               num_workers=workers, \n",
    "                                               collate_fn=utils.collate_fn)\n",
    "\n",
    "print(\"Create model\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "model = detection.__dict__['fasterrcnn_resnet50_fpn'](num_classes=num_classes, pretrained=False)\n",
    "model.to(device)\n",
    "model_without_ddp = model\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=lr, \n",
    "                            momentum=momentum, \n",
    "                            weight_decay=weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                                    milestones=lr_steps, \n",
    "                                                    gamma=lr_gamma)\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start_epoch = time.time()\n",
    "    train_one_epoch(\n",
    "        model, optimizer, data_loader, device, epoch, 20, writer, label_names\n",
    "    )\n",
    "    print(f\"Epoch time {time.time() - start_epoch}\")\n",
    "    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step=epoch)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    if save_every_num_epochs and epoch % save_every_num_epochs == 0:\n",
    "        utils.save_on_master({\n",
    "            'model': model_without_ddp.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'args': args,\n",
    "            'label_names': label_names},\n",
    "            os.path.join(writer.log_dir, 'model_{}.pth'.format(epoch))\n",
    "        )\n",
    "\n",
    "    if epoch % evaluate_every_num_epochs == 0:\n",
    "        evaluate(\n",
    "            model, data_loader_test, epoch, writer, draw_threshold,\n",
    "            label_names, num_draw_predictions, device=device\n",
    "        )\n",
    "\n",
    "# Save after training is done\n",
    "utils.save_on_master({\n",
    "    'model': model_without_ddp.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_scheduler': lr_scheduler.state_dict(),\n",
    "    'label_names': label_names},\n",
    "    os.path.join(writer.log_dir, 'model_finished.pth')\n",
    ")\n",
    "\n",
    "writer.close()\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
